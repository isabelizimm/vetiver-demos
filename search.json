[
  {
    "objectID": "coffee-explore-deploy/notebook-workflow.html",
    "href": "coffee-explore-deploy/notebook-workflow.html",
    "title": "Vetiver demos",
    "section": "",
    "text": "We previously wrote a pin to RSConnect that included a model to determine how many YouTube likes a Superbowl ad would receive. We can deploy that pin to different locations using a few helper functions.\n\nimport pins\nimport vetiver\n\nimport os\nimport rsconnect\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv())\n\napi_key = os.getenv(\"API_KEY\")\nrsc_url = os.getenv(\"RSC_URL\")\n\nboard = pins.board_rsconnect(api_key=api_key, server_url=rsc_url, allow_pickle_read=True)\n\nFrom here, we can move our API from locally hosted to other locations. Vetiver offers built-in functionality to deploy our model to Connect.\n\nconnect_server = rsconnect.api.RSConnectServer(url = rsc_url, api_key = api_key)\n\nvetiver.deploy_rsconnect(\n    connect_server = connect_server, \n    board = board, \n    pin_name = \"isabel.zimmerman/superbowl_rf\", \n    version = \"59869\")\n\nHowever, other cloud deployments may require a Dockerfile. For this workflow, we’ll need first write_app() to generate a dedicated app.py file to be stored inside our container, and then write_docker() to create a Dockerfile.\n\nvetiver.write_app(board=board, pin_name=\"isabel.zimmerman/superbowl_rf\")\nvetiver.write_docker(app_file=\"app.py\")"
  },
  {
    "objectID": "superbowl-intro-py/notebook.html",
    "href": "superbowl-intro-py/notebook.html",
    "title": "Vetiver demos",
    "section": "",
    "text": "Data scientists can still use the tools they are most comfortable with for the bulk of their workflow.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection, preprocessing, pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nimport rsconnect\nimport vetiver\nfrom vetiver import vetiver_pin_write, vetiver_endpoint\n\nimport os\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv())\n\napi_key = os.getenv(\"API_KEY\")\nrsc_url = os.getenv(\"RSC_URL\")\nnp.random.seed(500)\n\nWe can read in our data, and fit a pipeline that has both the preprocessing steps and the model.\n\nraw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')\ndf = pd.DataFrame(raw)\n\n\ndf = df[[\"like_count\", \"funny\", \"show_product_quickly\", \"patriotic\", \\\n    \"celebrity\", \"danger\", \"animals\"]].dropna()\nX, y = df.iloc[:,1:],df['like_count']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size=0.2)\n\nle = preprocessing.OrdinalEncoder().fit(X)\nrf = RandomForestRegressor().fit(le.transform(X_train), y_train)\n\n\nrf_pipe = pipeline.Pipeline([('label_encoder',le), ('random_forest', rf)])\n\n\n\n\nUsers first create a deployable model object, VetiverModel(). This holds all the pieces necessary to deploy the model later.\nIn R, you saw the equivalent, vetiver_model().\n\nv = vetiver.VetiverModel(\n    rf_pipe, \n    ptype_data=X_train, \n    model_name = \"isabel.zimmerman/superbowl_rf\"\n)\n\n\nimport pins \nboard = pins.board_rsconnect(api_key=api_key, server_url=rsc_url, allow_pickle_read=True)\n\nvetiver_pin_write(board, v)\n\nWriting pin:\nName: 'isabel.zimmerman/superbowl_rf'\nVersion: 20220809T143202Z-fd402\n\n\n\n\n\nNext, intialize the API endpoint with VetiverAPI(). To run the API locally, use .run()\nIn R, you saw the equivalents, vetiver_api() and pr_run().\n\napp = vetiver.VetiverAPI(v, check_ptype=True)\napp.run()\n\nThis is a great start to debug my API, but my end goal is to NOT run my model on my personal machine. We can instead deploy to a remote server, such as RStudio Connect. This will involve setting up a connection with the server and deploying our pinned model to RSConnect.\n\nconnect_server = rsconnect.api.RSConnectServer(url = rsc_url, api_key = api_key)\n\nWe can deploy our model, which is strongly linked to the version we just pinned above. Note: this model is already deployed, so no need to run this chunk again, unless we want to update our model.\n\nvetiver.deploy_rsconnect(\n    connect_server = connect_server, \n    board = board, \n    pin_name = \"isabel.zimmerman/superbowl_rf\", \n    version = \"59869\")\n\nWith the model deployed, we can interact with the API endpoint as if it were a model in memory.\n\nconnect_endpoint = vetiver_endpoint(\"https://colorado.rstudio.com/rsc/ads/predict\")\n\nresponse = vetiver.predict(data = X_test.head(5), endpoint = connect_endpoint)\nresponse\n\n\n\n\n\n  \n    \n      \n      prediction\n    \n  \n  \n    \n      0\n      452.581548\n    \n    \n      1\n      15054.536775\n    \n    \n      2\n      8830.437135\n    \n    \n      3\n      9872.934486\n    \n    \n      4\n      181.150403\n    \n  \n\n\n\n\nVetiver also helps make deployment easier for other cloud providers by offering functions to automatically write app.py files and Dockerfiles.\n\n# write app to be deployed within docker, or to other cloud provider\nvetiver.write_app(board, \"isabel.zimmerman/superbowl_rf\", version = \"59869\")\n\n# write Dockerfile\nvetiver.write_docker()"
  },
  {
    "objectID": "monitor-connect-py/monitor.html",
    "href": "monitor-connect-py/monitor.html",
    "title": "Monitor",
    "section": "",
    "text": "The vetiver framework offers functions to fluently compute, store, and plot model metrics. These functions are particularly suited to monitoring your model using multiple performance metrics over time.\nWhen a model is deployed, new data comes in over time, even if time is not a feature for prediction. Even if your model does not explicitly use any dates, a measure of time like a date can affect your model performance."
  },
  {
    "objectID": "monitor-connect-py/monitor.html#build-a-model",
    "href": "monitor-connect-py/monitor.html#build-a-model",
    "title": "Monitor",
    "section": "Build a model",
    "text": "Build a model\n\nfrom vetiver import VetiverModel\nfrom pins import board_folder\n\nmodel_board = board_folder(\".\", allow_pickle_read=True)\nv = VetiverModel.from_pin(model_board, \"cars\")"
  },
  {
    "objectID": "monitor-connect-py/monitor.html#compute-metrics",
    "href": "monitor-connect-py/monitor.html#compute-metrics",
    "title": "Monitor",
    "section": "Compute metrics",
    "text": "Compute metrics\nLet’s say we collect new data on fuel efficiency in cars and we want to monitor the performance of our model over time. We can compute multiple metrics at once over a certain time aggregation.\n\nimport vetiver\n\nimport pandas as pd\nfrom sklearn import metrics\nfrom datetime import timedelta\n\ncars = pd.read_csv(\"https://vetiver.rstudio.com/get-started/new-cars.csv\")\noriginal_cars = cars.iloc[:14, :].copy()\noriginal_cars[\"preds\"] = v.model.predict(\n    original_cars.drop(columns=[\"date_obs\", \"mpg\"])\n)\n\nmetric_set = [metrics.mean_absolute_error, \n  metrics.mean_squared_error, \n  metrics.r2_score]\n  \ntd = timedelta(weeks = 1)\n\noriginal_metrics = vetiver.compute_metrics(\n    data = original_cars, \n    date_var = \"date_obs\", \n    period = td, \n    metric_set = metric_set, \n    truth = \"mpg\", \n    estimate = \"preds\"\n)\n\noriginal_metrics\n\n\n\n\n\n  \n    \n      \n      index\n      n\n      metric\n      estimate\n    \n  \n  \n    \n      0\n      2022-03-24\n      7\n      mean_absolute_error\n      1.784605\n    \n    \n      1\n      2022-03-24\n      7\n      mean_squared_error\n      4.158348\n    \n    \n      2\n      2022-03-24\n      7\n      r2_score\n      0.679499\n    \n    \n      3\n      2022-03-31\n      7\n      mean_absolute_error\n      1.458550\n    \n    \n      4\n      2022-03-31\n      7\n      mean_squared_error\n      3.370279\n    \n    \n      5\n      2022-03-31\n      7\n      r2_score\n      0.892011"
  },
  {
    "objectID": "monitor-connect-py/monitor.html#pin-metrics",
    "href": "monitor-connect-py/monitor.html#pin-metrics",
    "title": "Monitor",
    "section": "Pin metrics",
    "text": "Pin metrics\nThe first time you pin monitoring metrics, you can write to a board as normal.\n\nmodel_board.pin_write(original_metrics, \"tree_metrics\", type = \"csv\")\n\nHowever, when adding new metrics measurements to your pin as you continue to gather new data and monitor, you may have dates that overlap with those already in the pin, depending on your monitoring strategy. You can choose how to handle overlapping dates with the overwrite argument.\n\n# dates overlap with existing metrics:\nnew_cars = cars.iloc[7:, :].copy()\nnew_cars[\"preds\"] = v.model.predict(\n    new_cars.drop(columns=[\"date_obs\", \"mpg\"])\n)\n\nnew_metrics = vetiver.compute_metrics(\n    data = new_cars, \n    date_var = \"date_obs\", \n    period = td, \n    metric_set = metric_set, \n    truth = \"mpg\", \n    estimate = \"preds\"\n)\n                    \nvetiver.pin_metrics(\n    model_board, \n    new_metrics, \n    \"tree_metrics\", \n    overwrite = True\n)"
  },
  {
    "objectID": "monitor-connect-py/monitor.html#plot-metrics",
    "href": "monitor-connect-py/monitor.html#plot-metrics",
    "title": "Monitor",
    "section": "Plot metrics",
    "text": "Plot metrics\nYou can visualize your set of computed metrics and your model’s performance.\n\nmonitoring_metrics = model_board.pin_read(\"tree_metrics\", version=\"20220809T163838Z-225f4\")\np = vetiver.plot_metrics(df_metrics = monitoring_metrics)\np.update_yaxes(matches=None)\np.write_image(\"../images/monitor.png\")\n\n\n\n\nplot of monitoring data"
  },
  {
    "objectID": "torch-new-models-py/notebook.html",
    "href": "torch-new-models-py/notebook.html",
    "title": "Vetiver demos",
    "section": "",
    "text": "Data scientists can still use the tools they are most comfortable with for the bulk of their workflow. Here, we can see using torch for a deep learning task of some random points.\n\nimport torch.nn as nn\nimport torch\nimport numpy as np\n\nimport vetiver\n\n\n# Hyper-parameters\ninput_size = 1\noutput_size = 1\nnum_epochs = 60\nlearning_rate = 0.001\n\n# # Toy dataset\nx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n                    [9.779], [6.182], [7.59], [2.167], [7.042], \n                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n\ny_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n                    [3.366], [2.596], [2.53], [1.221], [2.827], \n                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n\n# Linear regression model\nmodel = nn.Linear(input_size, output_size)\n\n# Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n# Train the model\nfor epoch in range(num_epochs):\n    # Convert numpy arrays to torch tensors\n    inputs = torch.from_numpy(x_train)\n    targets = torch.from_numpy(y_train)\n\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n\n\n\n\nJust like we saw with previous models we can create a deployable model object from our torch model and version with pins.\n\nv = vetiver.VetiverModel(\n    model = model, \n    save_ptype = True, \n    ptype_data=x_train,\n    model_name=\"torch\", \n    description=\"A regression model for testing purposes\"\n)\n\n\nimport pins\n\nboard = pins.board_temp(allow_pickle_read=True)\nvetiver.vetiver_pin_write(board, v)\n\nWriting pin:\nName: 'torch'\nVersion: 20220809T165227Z-999a5\n\n\n\n\n\nNext, we can make a local API endpoint with VetiverAPI() and start it with .run()\n\napi = vetiver.VetiverAPI(v, check_ptype=True)\napi.run()\n\n\n\n\nSometimes, you might use a model type that is not natively supported by Vetiver, or maybe you are writing a completely custom model that is not associated with any package at all. You are still able to deploy these models using a VetiverHandler()."
  }
]